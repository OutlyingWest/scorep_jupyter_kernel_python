{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a demonstrator of the Score-P Python Kernel\n",
    "This is the Score-P Python Kernel that allows you to execute Jupyter Notebooks with Score-P.\n",
    "\n",
    "The kernel supports the usual jupyter interactivity between cells but with some limitations (see \"General Limitations\").\n",
    "\n",
    "## Setup\n",
    "You can set up your Score-P environment by executing a cell that starts with the %%scorep_env magic command.\n",
    "\n",
    "You can set the Score-P Python binding arguments by executing a cell that starts with %%scorep_python_binding_arguments.\n",
    "\n",
    "## Usage\n",
    "Cells that should be executed with Score-P have to be marked with %%execute_with_scorep in the first line. Cells without that command are executed as ordinary Python processes.\n",
    "\n",
    "### Multi Cell Mode\n",
    "You can also treat multiple cells as one single cell by using the multi cell mode.\n",
    "\n",
    "Therefore you can mark the cells in the order you wish to execute them. Start the marking process by a cell that starts with the %%enable_multicellmode command.\n",
    "Now mark your cells by running them. Note that the cells will not be executed at this point but will be marked for later execution.\n",
    "You can stop the marking and execute all the marked cells by running a cell that starts with %%finalize_multicellmode command.\n",
    "This will execute all the marked cells orderly with Score-P. Note that the %%execute_with_scorep command has no effect in the multi cell mode.\n",
    "\n",
    "There is no \"unmark\" command available but you can abort the multicellmode by the %%abort_multicellmode command. Start your marking process again if you have marked your cells in the wrong order.\n",
    "\n",
    "The %%enable_multicellmode, %%finalize_multicellmode and %%abort_multicellmode commands should be run in an exclusive cell. Additional code in the cell will be ignored.\n",
    "\n",
    "### Presentation of Performance Data\n",
    "\n",
    "To inspect the collected performance data, use tools as Vampir (Trace) or Cube (Profile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set up SCORE-P Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score-P environment set successfully: {'SCOREP_ENABLE_TRACING': '1', 'SCOREP_ENABLE_PROFILING': '0', 'SCOREP_TOTAL_MEMORY': '3g'}"
     ]
    }
   ],
   "source": [
    "%%scorep_env\n",
    "SCOREP_ENABLE_TRACING=1\n",
    "SCOREP_ENABLE_PROFILING=0\n",
    "SCOREP_TOTAL_MEMORY=3g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score-P Python binding arguments set successfully: ['--noinstrumenter', '--noinstrumenter']"
     ]
    }
   ],
   "source": [
    "%%scorep_python_binding_arguments\n",
    "--noinstrumenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scorep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 1: Data Conjunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 1000000, 20\n",
    "df1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols)) for _ in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000Instrumentation results can be found in /home/visitor/Demonstrators/score-p_kernel/supplementary/example/scorep-20230630_1737_31341656626898"
     ]
    }
   ],
   "source": [
    "%%execute_with_scorep\n",
    "with scorep.instrumenter.enable():\n",
    "    # data conjunction\n",
    "    df5 = df1 + df2 +df3 + df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000Instrumentation results can be found in /home/visitor/Demonstrators/score-p_kernel/supplementary/example/scorep-20230630_1737_31353813505508"
     ]
    }
   ],
   "source": [
    "%%execute_with_scorep\n",
    "with scorep.instrumenter.enable():\n",
    "    # data conjunction\n",
    "    df5 = pd.eval(\"df1 + df2 +df3 + df4\", engine='numexpr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 2: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fairytales_demo.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import scorep\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%d/%m/%Y %H:%M:%S\",\n",
    "    level=logging.INFO)\n",
    "\n",
    "from utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "import numpy as numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from model import GPT, GPTconfig\n",
    "from trainer import Trainer, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the data set\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i:ch for i, ch in enumerate(chars)}\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "\n",
    "        x = torch.tensor(dix[:-1], dtype = torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype = torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execute_with_scorep\n",
    "\n",
    "with scorep.instrumenter.enable():\n",
    "    block_size = 32\n",
    "\n",
    "    text = open(\"./{}\".format(filename), \"r\").read()\n",
    "    train_dataset = CharDataset(text, block_size)\n",
    "\n",
    "    \n",
    "    mconf = GPTconfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                      n_layer=4, n_head=4, n_embd=256)\n",
    "    model = GPT(mconf)\n",
    "\n",
    "    tconf = TrainerConfig(max_epochs=1, batch_size=1024, learning_rate=0.01,\n",
    "                          lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                          num_workers=1)\n",
    "    trainer = Trainer(model, train_dataset, None, tconf)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()\n",
    "\n",
    "    torch.save(model.state_dict(), \"./saved_models/trained_gpt_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image info](trace_profile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The sun shone in the sky.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execute_with_scorep\n",
    "from utils import sample\n",
    "\n",
    "with scorep.instrumenter.enable():\n",
    "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "    completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "    print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Playground\n",
    "Feel free to add your code and to analyze it\n",
    "\n",
    "Begin a cell with:\n",
    "%%execute_with_scorep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scorep-python",
   "language": "python",
   "name": "scorep-python"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/plain",
   "name": "Any text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
